---
title: "The Velocity Problem"
date: 2024-12-12
description: "AI capability, products, and organizations all move at different speeds. What if the gap between them is structural?"
---

I keep noticing the same thing from different angles, and I'm not sure anyone's connecting them.

McKinsey publishes a report on agentic AI. The framing is: organizations need to move beyond experimentation, redesign their operating models, establish governance structures. The assumption underneath is that the gap between AI capability and organizational readiness is closable. You just need better transformation playbooks, cross-functional squads, CEO leadership.

Meanwhile, 77% of employees say AI has increased their workload, not decreased it. Nearly half are hiding their AI use from managers because they're afraid of being seen as lazy or incompetent. Products ship 18 months after acquisition into a world that's already moved on. Companies jump into AI from FOMO without understanding the workflows or behavior changes required.

These feel like they should be related, but nobody's treating them as the same problem.

## Reverse Salients

There's a concept in systems engineering called "reverse salients"—when technical components of a system advance but organizational, legal, and financial components lag behind and become bottlenecks. The bottleneck isn't a bug in the system. It's a structural feature. The components move at different speeds because they're governed by different forces.

I'm starting to wonder if that's what's happening here.

AI capability advances on one clock—research cycles, compute scaling, model releases. Products advance on another clock—roadmaps, integration, user testing. Organizations advance on a third clock—budgets, hiring, culture change, power dynamics. And the clocks aren't synchronized. Maybe they can't be.

By the time a company figures out who validates Claude's research, or how AI-drafted content flows through their approval process, or whose job it is to coordinate AI-augmented workflows, the capability has advanced twice. The expectation bar has shifted. And they're still structured like it's 2019.

## The Implementation Assumption

What makes me uneasy about the McKinsey framing—and the broader "AI transformation" genre—is that it treats this as an implementation problem. Get the operating model right, and the gap closes.

But what if the gap is structural? What if organizations fundamentally cannot restructure fast enough to absorb what's being built?

Not because they're doing it wrong. Because organizational change operates on a different timescale than AI advancement, period. The forces that govern how fast a company can reorganize—budgets, incentives, politics, culture, legal constraints—aren't accelerating just because the technology is.

Kissinger, Schmidt, and Huttenlocher wrote something that stuck with me: "Questions that appear urgent may be out of date by the time the relevant official participants have gathered to discuss them." That's not an adoption problem. That's a velocity problem.

## The Category Problem

And then there's a deeper layer I'm still working through.

There's a philosopher named David Gunkel who argues that our basic categories—person versus thing, tool versus agent, who's responsible for what—don't fit what AI actually is. We're trying to slot 21st century systems into 19th century conceptual boxes. The person/thing binary is cracking, and we don't have a replacement.

If he's right, then even if you solve the speed problem and the structural problem, you still have the category problem. We don't have shared language for what AI is in a way that maps to accountability, responsibility, governance. The frameworks assume a world where tools are tools and people are people. AI sits in a gray zone that our institutions weren't built to recognize.

## Fractures Everywhere

I don't have a clean answer here. But I'm increasingly skeptical of the framing that treats this as "adoption challenges" with "transformation playbooks."

The fractures are everywhere:

- Workers experiencing AI as burden, not relief
- Products shipping into contexts that have already moved
- Governance that's structurally reactive
- Organizations optimized for a world that no longer exists
- Categories that don't fit

And the common thread might be: everything is moving at different speeds, nobody has a shared picture, and we're using old maps for new territory.

Someone needs to be asking—not "how do we help organizations transform faster" but "how do we design knowing they can't?"

That's the question I keep coming back to. I don't think anyone's explicitly owning it yet.
